---
title: "Prediction Challenge"
output: html_notebook
---
# To Do List #
- LR Model
  - Model Assessment
    - Step wise
    - Cutoff Selection
    - Compare Step vs General Model
  - 

# Objective #
A supermarket is offering a new line of organic products.The supermarket's management wants to determine which customers are likely to purchase these products.

The supermarket has a customer loyalty program. As an initial buyer incentive plan, the supermarket provided coupons for the organic products to all of the loyalty program participants and collected data that includes whether these customers purchased any of the organic products.

The ORGANICS data set contains 13 variables and over 22,000 observations. The variables in the data set are  shown below with the appropriate roles and levels:

# Data Preprocessing #
## Data Extraction ##
```{r}
rm(list=ls())
setwd("/Users/jpferrer95/Google Drive/Offline/Spring 2018/BIS 348/Data")
rdata = read.csv("organics.csv")
rdata = rdata[complete.cases(rdata),]
```

In order to focus just on the data we have, and without making any wild assumptions, we have decided to eliminate all entries with missing (NA or Null) values. By doing so we avoid pitfalls in our analysis.After eliminating the empty cells, we can notice the number of entries decreased from 22223 observations to 18904 observations.

## Data Cleaning / Dummies ##
This part of the process is important because we are trying to decide which categorical variables to be taken into account in our predictive model. We gotta create dummy variables for the following variables: Clusters, genders, regions, tv regions and promo class

```{r}
options(scipen=999)

# Cluster Dummies 
rdata$isClusterA = ifelse(rdata$DemClusterGroup == 'A', 1, 0)
rdata$isClusterB = ifelse(rdata$DemClusterGroup == 'B', 1, 0)
rdata$isClusterC = ifelse(rdata$DemClusterGroup == 'C', 1, 0)
rdata$isClusterD = ifelse(rdata$DemClusterGroup == 'D', 1, 0)
rdata$isClusterE = ifelse(rdata$DemClusterGroup == 'E', 1, 0)
rdata$isClusterF = ifelse(rdata$DemClusterGroup == 'F', 1, 0)

# Gender Dummies 
rdata$isMale= ifelse(rdata$DemGender == 'M', 1, 0)
rdata$isFemale = ifelse(rdata$DemGender == 'F', 1, 0)

# Region Dummies
rdata$isMidland = ifelse(rdata$DemReg == 'Midlands', 1, 0)
rdata$isNorth = ifelse(rdata$DemReg == 'North', 1, 0)
rdata$isScottish = ifelse(rdata$DemReg == 'Scottish', 1, 0)
rdata$isSouthEast = ifelse(rdata$DemReg == 'South East', 1, 0)

# TV Region Dummmies
rdata$isBorder = ifelse(rdata$DemTVReg == 'Border', 1, 0)
rdata$isCScotland = ifelse(rdata$DemTVReg == 'C Scotland', 1, 0)
rdata$isEast = ifelse(rdata$DemTVReg == 'East', 1, 0)
rdata$isLondon = ifelse(rdata$DemTVReg == 'London', 1, 0)
rdata$isMidlands = ifelse(rdata$DemTVReg == 'Midlands', 1, 0)
rdata$isNEast = ifelse(rdata$DemTVReg == 'N East', 1, 0)
rdata$isNScot = ifelse(rdata$DemTVReg == 'N Scot', 1, 0)
rdata$isNWest = ifelse(rdata$DemTVReg == 'N West', 1, 0)
rdata$isSSEast = ifelse(rdata$DemTVReg == 'S & S East', 1, 0)
rdata$isSWest= ifelse(rdata$DemTVReg == 'S West', 1, 0)
rdata$isUlster = ifelse(rdata$DemTVReg == 'Ulster', 1, 0)
rdata$isWalesWest = ifelse(rdata$DemTVReg == 'Wales & West', 1, 0)

# Promotion Class Dummies
rdata$isGold = ifelse(rdata$PromClass == 'Gold', 1, 0)
rdata$isPlatinum = ifelse(rdata$PromClass == 'Platinum', 1, 0)
rdata$isSilver = ifelse(rdata$PromClass == 'Silver', 1, 0)
```

The *base case*: U Cluster, Unknown gender, South West region, Yorkshire TV region, Tin promotion class

## Data Partitioning ##
```{r}
cdata = rdata[,-c(1,5,6,7,8,9,13)]
set.seed(123)
ti = sample(nrow(cdata), floor(nrow(cdata)*0.6))
train.df = cdata[ti,]
valid.df = cdata[-ti,]
```

##### Logistic Regression Model ######
# Logistic Regression Model #
## Setting Up Model ##
```{r}
# install.packages("forecast")
# install.packages("caret")
library(forecast)
library(caret)

logReg = glm(TargetBuy~.,data = train.df, family = 'binomial')
logReg.pred = predict(logReg, newdata = valid.df, type = 'response')
pred = ifelse(logReg.pred > 0.5, 1, 0)

pred.df = data.frame(Actual = valid.df$TargetBuy, Prediction = pred,
                     Probability = logReg.pred )

#pred.df[order(-logReg.pred),]
confusionMatrix(pred, valid.df$TargetBuy)
```

## Model Assessment ##
```{r}
# Choose Cutoff #
accT = c()
for(cutoff in seq(0, 1, 0.01)){
    cm = confusionMatrix(ifelse(logReg.pred > cutoff, 1, 0), valid.df$TargetBuy)
    accT = c(accT, cm$overall[1])
}

plot(accT ~ seq(0, 1, 0.01), xlab = "Cutoff Value", ylab = "", type = "l", ylim = c(0,1))
lines(1-accT ~ seq(0,1,0.01), type = "l", lty = 2)
legend("bottomright", c("accuracy", "overall error"), lty = c(1,2), merge = F)
```

*Reduce number of Predictors*
```{r}
# Reduced amount of variables taken into account
# step.logReg = step(logReg, direction = 'both')
step.logReg.pred = predict(step.logReg, newdata = valid.df, type = 'response')

step.pred = ifelse(step.logReg.pred > 0.5, 1, 0)

step.pred.df = data.frame(Actual = valid.df$TargetBuy, Prediction = step.pred,
                          Probability = step.logReg.pred )

#pred.df[order(-logReg.pred),]
confusionMatrix(step.pred, valid.df$TargetBuy)

```
# Neural Network Model #
## Model Implementation ##
```{r}
library(neuralnet)
library(forecast)

# Normalize the data
nor = preProcess(cdata, method="range")
nn.data = predict(nor, cdata) 
str(nn.data)

train.df.nn = nn.data[ti,]
valid.df.nn = nn.data[-ti,]

# Getting the formula
n = names(train.df.nn)
f = as.formula(paste("TargetBuy~", paste(n[!n %in% "TargetBuy"], collapse = "+")))

# decide how many layers and nodes to use
organic.nn1 = neuralnet(f, data = train.df.nn, linear.output = F, hidden = 3,threshold = 0.1)
# organic.nn2 <- neuralnet(f, data = train.df.nn, linear.output = F, hidden = 5,threshold = 0.1)
# organic.nn3 <- neuralnet(f, data = train.df.nn, linear.output = F, hidden = c(5,5),threshold = 0.1)

organic.nn.pred1 = compute(organic.nn1,valid.df.nn[,-6] )

accuracy(unlist(organic.nn.pred1),valid.df.nn[,6])

library(caret)

organic.nn.pred1
confusionMatrix(unlist(organic.nn.pred1),valid.df.nn[,6])
```
# Classification Tree Model #
## Model Creation ##
```{r}
library(caret)
library(rpart)
library(rpart.plot)

default.ct = rpart(TargetBuy~., data = train.df, method = "class")
prp(default.ct, type = 1, extra = 1, under = TRUE, 
    split.font = 3, varlen = -10, 
    box.col = ifelse(default.ct$frame$var == "<leaf>", 'gray','white'))

default.pred = predict(default.ct, valid.df, type="class")


def.cm = confusionMatrix(default.pred, valid.df$TargetBuy, dnn=c("Prediction","Actual"))

# Pruning Trees
cv.ct = rpart(TargetBuy~., data = train.df, method = "class",
              cp = 0.00001)

printcp(cv.ct)

min.xerror = cv.ct$cptable[which.min(cv.ct$cptable[,"xerror"]), "CP"]

pruned.ct = prune(cv.ct, cp = min.xerror)
prp(pruned.ct, type = 1, extra = 1, split.font = 1, varlen = -10)

pruned.pred = predict(pruned.ct, valid.df, type="class")
pruned.pred

pruned.cm = confusionMatrix(pruned.pred, valid.df$TargetBuy)

rbind(Default = def.cm$overall[1], Pruned = pruned.cm$overall[1])

# Improved? Model using Less Variables
step.ct = rpart(TargetBuy ~ DemAffl+DemAge+isMale+isFemale+isMidland+isSouthEast+isNEast+isSWest, 
                data = train.df, method = "class", cp = 0.00001)

prp(step.ct, type = 1, extra = 1, under = TRUE, 
    split.font = 3, varlen = -10, 
    box.col = ifelse(default.ct$frame$var == "<leaf>", 'gray','white'))

step.ct.pred = predict(step.ct, valid.df, type="class")
step.ct.cm = confusionMatrix(step.ct.pred, valid.df$TargetBuy, dnn=c("Prediction","Actual"))

# Pruning Tree
min.xerror.p = step.ct$cptable[which.min(step.ct$cptable[,"xerror"]), "CP"]

step.pruned.ct = prune(step.ct, cp = min.xerror.p)
prp(step.pruned.ct, type = 1, extra = 1, split.font = 1, varlen = -10)

step.pruned.pred = predict(step.pruned.ct, valid.df, type="class")
step.pruned.pred

step.pruned.cm = confusionMatrix(step.pruned.pred, valid.df$TargetBuy)


rbind(Default_CT = step.ct.cm$overall[1], Pruned_CT = step.pruned.cm$overall[1],
      Step_Default_CT = def.cm$overall[1], Pruned_Step_CT = pruned.cm$overall[1])

# THE ONE WE WILL PICK GIVEN THIS IS: PRUNED_STEP_CT with acc = 0.82531
```
# Model Implementation #
## New Data Pre-Processing ##
```{r}
## New Data Pre-Processing ####
test.df = read.csv("scoreorganics.csv")

test.df = test.df[complete.cases(test.df),]
levels(test.df$DemClusterGroup)
options(scipen=999)

# Cluster Dummies 
test.df$isClusterA = ifelse(test.df$DemClusterGroup == 'A', 1, 0)
test.df$isClusterB = ifelse(test.df$DemClusterGroup == 'B', 1, 0)
test.df$isClusterC = ifelse(test.df$DemClusterGroup == 'C', 1, 0)
test.df$isClusterD = ifelse(test.df$DemClusterGroup == 'D', 1, 0)
test.df$isClusterE = ifelse(test.df$DemClusterGroup == 'E', 1, 0)
test.df$isClusterF = ifelse(test.df$DemClusterGroup == 'F', 1, 0)

# Gender Dummies 
test.df$isMale= ifelse(test.df$DemGender == 'M', 1, 0)
test.df$isFemale = ifelse(test.df$DemGender == 'F', 1, 0)

# Region Dummies
test.df$isMidland = ifelse(test.df$DemReg == 'Midlands', 1, 0)
test.df$isNorth = ifelse(test.df$DemReg == 'North', 1, 0)
test.df$isScottish = ifelse(test.df$DemReg == 'Scottish', 1, 0)
test.df$isSouthEast = ifelse(test.df$DemReg == 'South East', 1, 0)

# TV Region Dummmies
test.df$isBorder = ifelse(test.df$DemTVReg == 'Border', 1, 0)
test.df$isCScotland = ifelse(test.df$DemTVReg == 'C Scotland', 1, 0)
test.df$isEast = ifelse(test.df$DemTVReg == 'East', 1, 0)
test.df$isLondon = ifelse(test.df$DemTVReg == 'London', 1, 0)
test.df$isMidlands = ifelse(test.df$DemTVReg == 'Midlands', 1, 0)
test.df$isNEast = ifelse(test.df$DemTVReg == 'N East', 1, 0)
test.df$isNScot = ifelse(test.df$DemTVReg == 'N Scot', 1, 0)
test.df$isNWest = ifelse(test.df$DemTVReg == 'N West', 1, 0)
test.df$isSSEast = ifelse(test.df$DemTVReg == 'S & S East', 1, 0)
test.df$isSWest= ifelse(test.df$DemTVReg == 'S West', 1, 0)
test.df$isUlster = ifelse(test.df$DemTVReg == 'Ulster', 1, 0)
test.df$isWalesWest = ifelse(test.df$DemTVReg == 'Wales & West', 1, 0)

# Promotion Class Dummies
test.df$isGold = ifelse(test.df$PromClass == 'Gold', 1, 0)
test.df$isPlatinum = ifelse(test.df$PromClass == 'Platinum', 1, 0)
test.df$isSilver = ifelse(test.df$PromClass == 'Silver', 1, 0)

test.df = test.df[,-c(1,5,6,7,8,9)]
```

## Implementation of Best Model ##
```{r}

```

# Conclusion #

*CLASS NOTES*
*LIFT CHARTS*
The lift chart is basically used in order to see how many cases must be used (customers contacted) using our predictive model and see the benefit in prediction we can have. The lift chart inherently includes the random predictive model which means, if all the cases, or all the people are contacted we will find all the positive cases, but is this efficient. If we can just focus on a number of people, a lift chart is valuable because it allows you to see the which cut off of people should the company go for, without incurring on uneccesary costs or no valuable increase in predictability. Data need to be sorted accordingly to their probability of success, which allows us to focus on the highly predicted customers in our model. 

*Gain Table*
The gains table can show you the depth of the file which constitutes for the percentage of data sample being predicted. The cummulative pct of total response shows the percentage of positive (1) entries that will be explained by the percentage of data being sampled. 



